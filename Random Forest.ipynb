{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1c026e",
   "metadata": {},
   "source": [
    "# Предобработка данных для обучения Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7ad41",
   "metadata": {},
   "source": [
    "### Загрузка данных и первоначальная обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcfef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Шаг 1: Загрузка данных...\n",
      "Шаг 2: Подготовка данных...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Шаг 1: Загрузка данных...\")\n",
    "df_train = pq.read_table(\"train_ai_comp_final_dp.parquet\").to_pandas()\n",
    "\n",
    "# Подготовка данных\n",
    "print(\"Шаг 2: Подготовка данных...\")\n",
    "X = df_train.drop([\"sample_ml_new\", \"feature642\", \"feature756\", \"target\"], axis=1)\n",
    "y = df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b0a685",
   "metadata": {},
   "source": [
    "### Сэмплирование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1267977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Шаг 3: Применение сэмплирования...\n",
      "До сэмплирования: Counter({0: 501078, 1: 18537})\n",
      "После сэмплирования:  Counter({0: 18537, 1: 18537})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Применение SMOTE для сэмплирования данных\n",
    "print(\"Шаг 3: Применение сэмплирования...\")\n",
    "# Подсчет количества примеров в каждом классе\n",
    "print(\"До сэмплирования:\", Counter(y))\n",
    "\n",
    "\n",
    "# Создание пайплайна с oversampling и undersampling\n",
    "pipeline = Pipeline([\n",
    "    #('oversample', SMOTE(sampling_strategy=1)),  # Увеличение примеров класса меньшинства до 50% от размера класса большинства\n",
    "    ('undersample', RandomUnderSampler(sampling_strategy=1.0))  # Уменьшение примеров класса большинства до размера класса меньшинства\n",
    "])\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X, y)\n",
    "\n",
    "print(\"После сэмплирования: \", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56790d",
   "metadata": {},
   "source": [
    "### Выявление наиболее значимых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ded17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "print(\"Шаг 4: Выявление наиболее значимых признаков...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_resampled, y_resampled)\n",
    "feature_importances = rf_model.feature_importances_\n",
    "important_features_indices = feature_importances.argsort()[-100:][::-1]  # Выбираем 100 наиболее значимых признаков\n",
    "\n",
    "# Использование только наиболее значимых признаков\n",
    "X_important = X_resampled.iloc[:, important_features_indices]\n",
    "print(important_features_indices)\n",
    "\n",
    "# Разделение данных на обучающий и валидационный наборы\n",
    "print(\"Шаг 5: Разделение данных на обучающий и валидационный наборы...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_important, y_resampled, test_size=0.25, random_state=42)\n",
    "\n",
    "# Сохранение модели\n",
    "print(\"Шаг 6: Сохранение модели...\")\n",
    "joblib.dump(rf_model, 'random_forest_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe9442",
   "metadata": {},
   "source": [
    "### Обучение модели Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация и обучение модели Random Forest\n",
    "print(\"Шаг 7: Инициализация и обучение модели Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание на валидационном наборе\n",
    "print(\"Шаг 8: Предсказание на валидационном наборе...\")\n",
    "y_pred = rf_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a074817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление метрик\n",
    "print(\"Шаг 9: Вычисление метрик...\")\n",
    "precision = precision_score(y_val, y_pred)\n",
    "recall = recall_score(y_val, y_pred)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "roc_auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "# Сохранение модели\n",
    "print(\"Шаг 10: Сохранение новой модели...\")\n",
    "joblib.dump(rf_model, 'random_forest_model_after.model')\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Метрики:\")\n",
    "print(f\"  Precision: {precision}\")\n",
    "print(f\"  Recall: {recall}\")\n",
    "print(f\"  F1 Score: {f1}\")\n",
    "print(f\"  ROC AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86351e",
   "metadata": {},
   "source": [
    "# Проверка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cbb7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ynicp\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "node array from the pickle has an incompatible dtype:\n- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Загрузка модели\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom_forest_model_after.model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(loaded_model))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Загрузка тестовых данных\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[0;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1212\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\numpy_pickle.py:402\u001b[0m, in \u001b[0;36mNumpyUnpickler.load_build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_build\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    395\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called to set the state of a newly created object.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    We capture it to replace our place-holder objects, NDArrayWrapper or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    NDArrayWrapper is used for backward compatibility with joblib <= 0.9.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     \u001b[43mUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_build\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# For backward compatibility, we support NDArrayWrapper objects.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], (NDArrayWrapper, NumpyArrayWrapper)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\pickle.py:1705\u001b[0m, in \u001b[0;36m_Unpickler.load_build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1703\u001b[0m setstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(inst, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1705\u001b[0m     \u001b[43msetstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1707\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:714\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.__setstate__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msklearn\\tree\\_tree.pyx:1418\u001b[0m, in \u001b[0;36msklearn.tree._tree._check_node_ndarray\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: node array from the pickle has an incompatible dtype:\n- expected: {'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity', 'n_node_samples', 'weighted_n_node_samples', 'missing_go_to_left'], 'formats': ['<i8', '<i8', '<i8', '<f8', '<f8', '<i8', '<f8', 'u1'], 'offsets': [0, 8, 16, 24, 32, 40, 48, 56], 'itemsize': 64}\n- got     : [('left_child', '<i8'), ('right_child', '<i8'), ('feature', '<i8'), ('threshold', '<f8'), ('impurity', '<f8'), ('n_node_samples', '<i8'), ('weighted_n_node_samples', '<f8')]"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Загрузка модели\n",
    "loaded_model = joblib.load('random_forest_model_after.model')\n",
    "print(type(loaded_model))\n",
    "# Загрузка тестовых данных\n",
    "file_path = \"test_sber.parquet\"\n",
    "file_name, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "needToDelete = [\"feature642\", \"feature756\", \"sample_ml_new\"]\n",
    "\n",
    "df = pq.read_table(file_path).to_pandas()\n",
    "\n",
    "df = df.drop(needToDelete, axis=1)\n",
    "\n",
    "#feature_indices = [1, 4, 43, 67, 88, 89, 90, 91, 93, 96, 98, 119, 124, 129, 130, 137, 195, 262, 263, 270, 282, 283, 284, 286, 287, 288, 309, 313, 342, 349, 350, 357, 358, 359, 385, 395, 396, 402, 432, 434, 437, 438, 440, 441, 445, 461, 464, 467, 472, 479, 481, 482, 485, 489, 529, 548, 614, 667, 674, 675, 692, 720, 726, 738, 741, 744, 745, 752, 753, 780, 791, 797, 808, 810, 813, 852, 859, 862, 863, 852, 859, 862, 863, 852, 859, 862, 863, 852, 859, 862, 863, 852, 859, 862, 863, 852, 859, 862, 863, 852, 859, 862, 863, 852, 859, 862, 863, 986, 992, 996, 998, 1000, 1003, 1004, 1074, 1076, 116, 341, 443, 444, ]\n",
    "feature_names = loaded_model.feature_names_in_\n",
    "#feature_names = np.append(feature_names, \"id\")\n",
    "\n",
    "print(\"Идет удаление ненужных признаков...\")\n",
    "df_test = df[feature_names]\n",
    "print(\"Удаление завершено\\n\")\n",
    "\n",
    "X_test = df\n",
    "\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# Предсказание\n",
    "predictions = loaded_model.predict_proba(df_test)[:, 1]\n",
    "\n",
    "rounded_predictions = [round(pred) for pred in predictions]\n",
    "\n",
    "count_of_1 = sum(predictions)\n",
    "print(\"Количество 1: \", count_of_1)\n",
    "print(\"Количество всего: \", len(predictions))\n",
    "\n",
    "# Создание DataFrame с предсказаниями и id\n",
    "df_predictions = pd.DataFrame({'id': df['id'],'target_bin': rounded_predictions, 'target_prob': predictions})\n",
    "\n",
    "# Сохранение предсказаний в CSV файл\n",
    "df_predictions.to_csv('test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf5acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
